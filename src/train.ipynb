{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <CENTER>Tweet mood analysis</CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "from preprocessing import data_preprocessing, external_data\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional,  Flatten, Input, GRU\n",
    "from keras.layers import Convolution1D, MaxoutDense, GlobalMaxPooling1D, Input, Conv1D, MaxPooling1D, Flatten, ConvLSTM2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "EMBEDDING_FILE = \"../GoogleNews-vectors-negative300.bin\"\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "#corpora_external=\"../external_data.txt\"\n",
    "corpora_train=\"../data/train.txt\"\n",
    "corpora_test=\"../data/test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_extr, y_train_extr = external_data(corpora_external)\n",
    "x_train, y_train = data_preprocessing(corpora_train, 'True')\n",
    "x_test= data_preprocessing(corpora_test, 'False')\n",
    "\n",
    "\n",
    "#x_train=pd.concat([x_train_extr,x_train_sem ])\n",
    "#y_train=pd.concat([y_train_extr,y_train_sem ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the tweets and get the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=' ')\n",
    "all_tweet = x_train.append(x_test)\n",
    "tokenizer.fit_on_texts(all_tweet)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "sequences = sequences_train + sequences_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the max tweet sequence and pad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 0\n",
    "for elt in sequences:\n",
    "\tif len(elt) > MAX_SEQUENCE_LENGTH:\n",
    "\t\tMAX_SEQUENCE_LENGTH = len(elt)\n",
    "\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train = np.arange(data_train.shape[0])\n",
    "data_train = data_train[indices_train]\n",
    "\n",
    "indices_test = np.arange(data_test.shape[0])\n",
    "data_test = data_test[indices_test]\n",
    "\n",
    "#indices_train = np.arange(data_train.shape[0])\n",
    "#data_train = data_train[indices_train]\n",
    "#labels=sorted(list(set(y_train.tolist())))\n",
    "\n",
    "#one_hot=np.zeros((len(labels),len(labels)),dtype=int)\n",
    "#np.fill_diagonal(one_hot,1)\n",
    "#label_dict=dict(zip(labels,one_hot))\n",
    "#y_train = y_train.apply(lambda y:label_dict[y]).tolist()\n",
    "\n",
    "\n",
    "nb_words=len(word_index)+1\n",
    "\n",
    "y_train = to_categorical(np.asarray(y_train), 4)\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "oov=[]\n",
    "oov.append((np.random.rand(EMBEDDING_DIM) * 2.0) - 1.0)\n",
    "oov = oov / np.linalg.norm(oov)\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    else:\n",
    "        embedding_matrix[i] = oov\n",
    "\n",
    "split_idx = int(len(x_train)*0.999999999)\n",
    "x_train, x_val = data_train[:split_idx], data_train[split_idx:]\n",
    "y_train, y_val = y_train [:split_idx], y_train[split_idx:]\n",
    "\n",
    "#print('training set: ' + str(len(x_train)) + ' samples')\n",
    "#print('validation set: ' + str(len(x_val)) + ' samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True, name='embedding_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(x_train, y_train, x_val, y_val, embedding_layer):\n",
    "\n",
    "\tmodel1 = Sequential()\n",
    "\tmodel1.add(embedding_layer)\n",
    "\tmodel1.add(Dropout(0.5))\n",
    "\tmodel1.add(GRU(128))\n",
    "\tmodel1.add(Dropout(0.5))\n",
    "\tmodel1.add(Dense(32, activation='relu'))\n",
    "\tmodel1.add(Dropout(0.2))\n",
    "\tmodel1.add(Dense(4, activation='softmax'))\n",
    "\tmodel1.compile(loss='categorical_crossentropy',\n",
    "\t\t\t      optimizer='Adam',\n",
    "\t\t\t      metrics=['acc'])\n",
    "\tmodel1.summary()\n",
    "#\tearly_stopping = EarlyStopping(patience = 2)\n",
    "#\tmodel_checkpoint = ModelCheckpoint(\"models\", save_best_only = True, save_weights_only = True)\n",
    "\tmodel1.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=50, epochs=3,  verbose=1)\n",
    "\tmodel1.save(\"./model1.h5\")\n",
    "\n",
    "\treturn model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(x_train, y_train, x_val, y_val, embedding_layer):\n",
    "\tmodel2 = Sequential()\n",
    "\tmodel2.add(embedding_layer)\n",
    "\t#model2.add(Bidirectional(LSTM(256,return_sequences=True)))\n",
    "\tmodel2.add(Dropout(0.5))\n",
    "\tmodel2.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
    "\t#model2.add(Dropout(0.5))\n",
    "#\tmodel2.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
    "\tmodel2.add(Dropout(0.5))\n",
    "        #^Bmodel2.add(Dense(256,name='dense1'))\n",
    "        #model2.add(LeakyReLU(alpha=0.05))\n",
    "\t# model2.add(Dropout(0.5))\n",
    "\t#model2.add(Dense(128,activation='relu', name='dense2'))\n",
    "\t#model2.add(LeakyReLU(alpha=0.05))\n",
    "\t#model2.add(Dropout(0.5))\n",
    "\tmodel2.add(Dense(64, activation='relu', name='dense3'))\n",
    "\t#model2.add(LeakyReLU(alpha=0.05))\n",
    "\tmodel2.add(Flatten())\n",
    "\tmodel2.add(Dense(4,activation='softmax',name='output_6'))\n",
    "\tmodel2.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "\tmodel2.summary()\n",
    "\tmodel2.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=50,  verbose=1)\n",
    "\tmodel2.save(\"./model2.h5\")\n",
    "    \n",
    "\treturn model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model CNN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN1 (x_train, y_train, x_val, y_val, embedding_layer):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(embedding_layer)\n",
    "\tmodel.add(Convolution1D(nb_filter=128, filter_length=5, border_mode='valid', activation='relu'))\n",
    "\tmodel.add(GlobalMaxPooling1D())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(Dense(4, activation='softmax'))\n",
    "\tmodel.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['acc'])\n",
    "\tmodel.summary()\n",
    "\tmodel.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=5, batch_size=100)\n",
    "\tmodel.save(\"./model3.h5\")\n",
    "\t\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model CNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN2 (x_train, y_train, x_val, y_val, embedding_layer ):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(embedding_layer)\n",
    "\tmodel.add(Convolution1D(64, 3, border_mode='same'))\n",
    "\tmodel.add(Convolution1D(32, 3, border_mode='same'))\n",
    "\tmodel.add(Convolution1D(16, 3, border_mode='same'))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(180,activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(4,activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\tmodel.summary()\n",
    "\tmodel.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=3, batch_size=100)\n",
    "\tmodel.save(\"./model4.h5\")\n",
    "    \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model1(x_train, y_train, x_val, y_val,  embedding_layer)\n",
    "#model=model2(x_train, y_train, x_val, y_val,  embedding_layer)\n",
    "#model=CNN1(x_train, y_train, x_val, y_val, embedding_layer)\n",
    "#model=CNN2(x_train, y_train, x_val, y_val, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cvscores = []\n",
    "kf=KFold(n_splits=5)\n",
    "for epoch in range(1,nb_epochs):\n",
    "\t\n",
    "\tprint (\"======= epoch =\", epoch)\n",
    "\ti=1\n",
    "\tcvscores = []\n",
    "\tfor train_index, test_index in kf.split(data_train):\n",
    "\t\tx_train_k, x_val_k = data_train[train_index], data_train[test_index]\n",
    "\t\ty_train_k, y_val_k = y_train[train_index], y_train[test_index]\n",
    "\t#\tprint(\"======================= Fold\", i\n",
    "\t\tmodel=model2(x_train_k, y_train_k, epoch,  embedding_layer)\n",
    "\t\tscores = model.evaluate(x_val_k, y_val_k, verbose=0)\n",
    "\t\tprint(\"Fold\", i,  \"-->\", model.metrics_names[1], scores[1]*100)\n",
    "#\t\tprint(\"%s: %.2f%%\" % (model1.metrics_names[1], scores[1]*100))\n",
    "\t\tcvscores.append(scores[1] * 100)\n",
    "\t\ti=i+1\n",
    "\t\tdel model\n",
    "\t\tgc.collect()\n",
    "\t\t#keras.clear_session()\n",
    "\t\t#gc.collect()\n",
    "\tprint(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./test.txt\", \"w\")\n",
    "f.write(\"id\\tturn1\\tturn2\\tturn3\\tlabel\\n\")\n",
    "#print (\"id\\tturn1\\tturn2\\tturn3\\tlabel\")\n",
    "nn_model=load_model(\"./model1.h5\")\n",
    "r = nn_model.predict(data_test)\n",
    "print(r)\n",
    "data = pd.read_csv(\"/home/bouche_a/semeval2019/data/test.txt\", sep='\\t', encoding='utf-8',     names=['id','turn1','turn2','turn3'])\n",
    " \n",
    "for d in range(1,len(data)):\n",
    "\ti=d-1 \n",
    "\tidx=numpy.argmax(r[i])\n",
    "\tif (idx==0):\n",
    "\t\tlabel=\"angry\"\n",
    "\telif(idx==1):\n",
    "\t\tlabel=\"happy\"\n",
    "\telif(idx==3):\n",
    "\t\tlabel=\"others\"\n",
    "\telif(idx==2):\n",
    "\t\t label=\"sad\"\n",
    "\tf.write(str(data[\"id\"][d])+\"\\t\"+str(data[\"turn1\"][d])+\"\\t\"+str(data[\"turn2\"][d])+\"\\t\"+str(data[\"turn3\"][d])+\"\\t\"+label+\"\\n\")\n",
    "#\tprint(str(data[\"id\"][d])+\"\\t\"+str(data[\"turn1\"][d])+\"\\t\"+str(data[\"turn2\"][d])+\"\\t\"+str (data[\"turn3\"][d])+\"\\t\"+label)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
