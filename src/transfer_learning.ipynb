{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "import utils\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Recreate vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../dataset/tweets/tweets_emotion_6/emotion_6-processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ_DIST_FILE = '../dataset/tweets/tweets_polarity_2/tweets_pos_neg_train-processed-freqdist.pkl'\n",
    "GLOVE_FILE = '../dataset/embedding/glove-seeds.txt'\n",
    "dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "np.random.seed(1337)\n",
    "vocab_size = 90000\n",
    "max_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the original vocab\n",
    "vocab = utils.top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Preprocess Emotion 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(tweet):\n",
    "    \"\"\"\n",
    "    Generates a feature vector for each tweet where each word is\n",
    "    represented by integer index based on rank in vocabulary.\n",
    "    \"\"\"\n",
    "    words = tweet.split()\n",
    "    feature_vector = []\n",
    "    for i in range(len(words) - 1):\n",
    "        word = words[i]\n",
    "        if vocab.get(word) is not None:\n",
    "            feature_vector.append(vocab.get(word))\n",
    "    if len(words) >= 1:\n",
    "        if vocab.get(words[-1]) is not None:\n",
    "            feature_vector.append(vocab.get(words[-1]))\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def process_tweets(csv_file, test_file=True):\n",
    "    \"\"\"\n",
    "    Generates training X, y pairs.\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    print('Generating feature vectors')\n",
    "    with open(csv_file, 'r') as csv:\n",
    "        lines = csv.readlines()\n",
    "        total = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            if test_file:\n",
    "                tweet_id, tweet = line.split(',')\n",
    "            else:\n",
    "                tweet_id, sentiment, tweet = line.split(',')\n",
    "            feature_vector = get_feature_vector(tweet)\n",
    "            if test_file:\n",
    "                tweets.append(feature_vector)\n",
    "            else:\n",
    "                tweets.append(feature_vector)\n",
    "                labels.append(int(sentiment))\n",
    "            utils.write_status(i + 1, total)\n",
    "    print('\\n')\n",
    "    return tweets, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EMOTION_6 = '../dataset/tweets/tweets_emotion_6/emotion_6-processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "Processing 383456/416809rocessing 63344/416809Processing 159124/416809Processing 161323/416809Processing 184718/416809Processing 226293/416809Processing 276463/416809Processing 309057/416809Processing 325654/416809Processing 374031/416809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 387310/416809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 398018/416809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 409334/416809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 411699/416809"
     ]
    }
   ],
   "source": [
    "tweets_emo_6, labels_emo_6 = process_tweets(TRAIN_EMOTION_6, test_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_emo_6 = to_categorical(labels_emo_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_emo_6 = pad_sequences(tweets_emo_6, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.random.permutation(tweets_emo_6.shape[0])\n",
    "tweets_emo_6 = tweets_emo_6[shuffled_indices]\n",
    "labels_emo_6 = labels_emo_6[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Preprocess Emotion 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EMOTION_4 = '../dataset/tweets/tweets_emotion_4/tweets_emotions_train-processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "Processing 14182/30160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 17998/30160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 22031/30160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 26083/30160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 30068/30160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_emo_4, labels_emo_4 = process_tweets(TRAIN_EMOTION_4, test_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_emo_4 = to_categorical(labels_emo_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_emo_4 = pad_sequences(tweets_emo_4, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.random.permutation(tweets_emo_4.shape[0])\n",
    "tweets_emo_4 = tweets_emo_4[shuffled_indices]\n",
    "labels_emo_4 = labels_emo_4[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transfer learning - (polarity_2 to emotion_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load model (positive/negative CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "PATH_MODEL_POS_NEG = '../models/base_model/4cnn-04-0.342-0.851-0.389-0.828.hdf5'\n",
    "transfer_model = load_model(PATH_MODEL_POS_NEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 200)           18000200  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 38, 600)           360600    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 36, 300)           540300    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 34, 150)           135150    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 32, 75)            33825     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               1440600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 601       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 20,511,276\n",
      "Trainable params: 20,511,276\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut last layers\n",
    "transfer_model.pop()\n",
    "transfer_model.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze layers\n",
    "for layer in transfer_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add news layers\n",
    "transfer_model.add(Dense(400, name='dense_2'))\n",
    "transfer_model.add(Activation('relu', name='activation_2'))\n",
    "\n",
    "transfer_model.add(Dense(6, name='dense_3'))\n",
    "transfer_model.add(Activation('softmax', name='activation_3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Fine tunning and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "#adam = optimizers.Adam(lr=5)\n",
    "transfer_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 200)           18000200  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 38, 600)           360600    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 36, 300)           540300    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 34, 150)           135150    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 32, 75)            33825     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               1440600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 2406      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 20,753,481\n",
      "Trainable params: 242,806\n",
      "Non-trainable params: 20,510,675\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./transfer_pol2_emo6/entire_corpus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "filepath = \"./transfer_pol2_emo6/entire_corpus/{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 375128 samples, validate on 41681 samples\n",
      "Epoch 1/20\n",
      "375128/375128 [==============================] - 388s 1ms/step - loss: 1.3640 - acc: 0.5028 - val_loss: 1.3349 - val_acc: 0.5098\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.36396, saving model to ./transfer_pol2_emo6/entire_corpus/01-1.364-0.503-1.335-0.510.hdf5\n",
      "Epoch 2/20\n",
      "375128/375128 [==============================] - 353s 941us/step - loss: 1.3475 - acc: 0.5069 - val_loss: 1.3285 - val_acc: 0.5113\n",
      "\n",
      "Epoch 00002: loss improved from 1.36396 to 1.34755, saving model to ./transfer_pol2_emo6/entire_corpus/02-1.348-0.507-1.328-0.511.hdf5\n",
      "Epoch 3/20\n",
      "375128/375128 [==============================] - 349s 930us/step - loss: 1.3438 - acc: 0.5078 - val_loss: 1.3226 - val_acc: 0.5119\n",
      "\n",
      "Epoch 00003: loss improved from 1.34755 to 1.34383, saving model to ./transfer_pol2_emo6/entire_corpus/03-1.344-0.508-1.323-0.512.hdf5\n",
      "Epoch 4/20\n",
      "375128/375128 [==============================] - 353s 942us/step - loss: 1.3408 - acc: 0.5087 - val_loss: 1.3197 - val_acc: 0.5139\n",
      "\n",
      "Epoch 00004: loss improved from 1.34383 to 1.34083, saving model to ./transfer_pol2_emo6/entire_corpus/04-1.341-0.509-1.320-0.514.hdf5\n",
      "Epoch 5/20\n",
      "375128/375128 [==============================] - 349s 930us/step - loss: 1.3391 - acc: 0.5096 - val_loss: 1.3182 - val_acc: 0.5144\n",
      "\n",
      "Epoch 00005: loss improved from 1.34083 to 1.33906, saving model to ./transfer_pol2_emo6/entire_corpus/05-1.339-0.510-1.318-0.514.hdf5\n",
      "Epoch 6/20\n",
      "375128/375128 [==============================] - 350s 933us/step - loss: 1.3379 - acc: 0.5098 - val_loss: 1.3175 - val_acc: 0.5143\n",
      "\n",
      "Epoch 00006: loss improved from 1.33906 to 1.33789, saving model to ./transfer_pol2_emo6/entire_corpus/06-1.338-0.510-1.317-0.514.hdf5\n",
      "Epoch 7/20\n",
      "375128/375128 [==============================] - 361s 962us/step - loss: 1.3361 - acc: 0.5103 - val_loss: 1.3158 - val_acc: 0.5146\n",
      "\n",
      "Epoch 00007: loss improved from 1.33789 to 1.33608, saving model to ./transfer_pol2_emo6/entire_corpus/07-1.336-0.510-1.316-0.515.hdf5\n",
      "Epoch 8/20\n",
      "375128/375128 [==============================] - 349s 932us/step - loss: 1.3352 - acc: 0.5105 - val_loss: 1.3156 - val_acc: 0.5136\n",
      "\n",
      "Epoch 00008: loss improved from 1.33608 to 1.33516, saving model to ./transfer_pol2_emo6/entire_corpus/08-1.335-0.511-1.316-0.514.hdf5\n",
      "Epoch 9/20\n",
      "375128/375128 [==============================] - 348s 927us/step - loss: 1.3347 - acc: 0.5104 - val_loss: 1.3127 - val_acc: 0.5153\n",
      "\n",
      "Epoch 00009: loss improved from 1.33516 to 1.33471, saving model to ./transfer_pol2_emo6/entire_corpus/09-1.335-0.510-1.313-0.515.hdf5\n",
      "Epoch 10/20\n",
      "375128/375128 [==============================] - 346s 923us/step - loss: 1.3336 - acc: 0.5107 - val_loss: 1.3126 - val_acc: 0.5154\n",
      "\n",
      "Epoch 00010: loss improved from 1.33471 to 1.33364, saving model to ./transfer_pol2_emo6/entire_corpus/10-1.334-0.511-1.313-0.515.hdf5\n",
      "Epoch 11/20\n",
      "375128/375128 [==============================] - 348s 927us/step - loss: 1.3337 - acc: 0.5108 - val_loss: 1.3131 - val_acc: 0.5153\n",
      "\n",
      "Epoch 00011: loss did not improve from 1.33364\n",
      "Epoch 12/20\n",
      "375128/375128 [==============================] - 345s 919us/step - loss: 1.3330 - acc: 0.5110 - val_loss: 1.3130 - val_acc: 0.5155\n",
      "\n",
      "Epoch 00012: loss improved from 1.33364 to 1.33304, saving model to ./transfer_pol2_emo6/entire_corpus/12-1.333-0.511-1.313-0.515.hdf5\n",
      "Epoch 13/20\n",
      "375128/375128 [==============================] - 344s 918us/step - loss: 1.3300 - acc: 0.5118 - val_loss: 1.3090 - val_acc: 0.5158\n",
      "\n",
      "Epoch 00013: loss improved from 1.33304 to 1.33001, saving model to ./transfer_pol2_emo6/entire_corpus/13-1.330-0.512-1.309-0.516.hdf5\n",
      "Epoch 14/20\n",
      "375128/375128 [==============================] - 350s 933us/step - loss: 1.3284 - acc: 0.5123 - val_loss: 1.3091 - val_acc: 0.5159\n",
      "\n",
      "Epoch 00014: loss improved from 1.33001 to 1.32843, saving model to ./transfer_pol2_emo6/entire_corpus/14-1.328-0.512-1.309-0.516.hdf5\n",
      "Epoch 15/20\n",
      "375128/375128 [==============================] - 341s 909us/step - loss: 1.3279 - acc: 0.5115 - val_loss: 1.3081 - val_acc: 0.5161\n",
      "\n",
      "Epoch 00015: loss improved from 1.32843 to 1.32789, saving model to ./transfer_pol2_emo6/entire_corpus/15-1.328-0.512-1.308-0.516.hdf5\n",
      "Epoch 16/20\n",
      "375128/375128 [==============================] - 345s 919us/step - loss: 1.3282 - acc: 0.5116 - val_loss: 1.3071 - val_acc: 0.5167\n",
      "\n",
      "Epoch 00016: loss did not improve from 1.32789\n",
      "Epoch 17/20\n",
      "375128/375128 [==============================] - 355s 945us/step - loss: 1.3279 - acc: 0.5119 - val_loss: 1.3071 - val_acc: 0.5176\n",
      "\n",
      "Epoch 00017: loss did not improve from 1.32789\n",
      "Epoch 18/20\n",
      "375128/375128 [==============================] - 352s 938us/step - loss: 1.3271 - acc: 0.5123 - val_loss: 1.3080 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00018: loss improved from 1.32789 to 1.32708, saving model to ./transfer_pol2_emo6/entire_corpus/18-1.327-0.512-1.308-0.516.hdf5\n",
      "Epoch 19/20\n",
      "375128/375128 [==============================] - 355s 946us/step - loss: 1.3264 - acc: 0.5125 - val_loss: 1.3062 - val_acc: 0.5168\n",
      "\n",
      "Epoch 00019: loss improved from 1.32708 to 1.32643, saving model to ./transfer_pol2_emo6/entire_corpus/19-1.326-0.512-1.306-0.517.hdf5\n",
      "Epoch 20/20\n",
      "375128/375128 [==============================] - 353s 941us/step - loss: 1.3255 - acc: 0.5125 - val_loss: 1.3066 - val_acc: 0.5170\n",
      "\n",
      "Epoch 00020: loss improved from 1.32643 to 1.32551, saving model to ./transfer_pol2_emo6/entire_corpus/20-1.326-0.513-1.307-0.517.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f53bdf907f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "transfer_model.fit(tweets_emo_6, labels_emo_6, batch_size=128, epochs=20, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer learning - (emotion_6 to emotion_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Load model (positive/negative CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../models/transfer_pol2_emo6/entire_corpus/20-1.326-0.513-1.307-0.517.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "PATH_MODEL_EMO_6 = '../models/transfer_pol2_emo6/entire_corpus/20-1.326-0.513-1.307-0.517.hdf5'\n",
    "transfer_model = load_model(PATH_MODEL_EMO_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 200)           18000200  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 38, 600)           360600    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 36, 300)           540300    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 34, 150)           135150    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 32, 75)            33825     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               1440600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 2406      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 20,753,481\n",
      "Trainable params: 242,806\n",
      "Non-trainable params: 20,510,675\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut last layers\n",
    "transfer_model.pop()\n",
    "transfer_model.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze layers\n",
    "for layer in transfer_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add news layers\n",
    "transfer_model.add(Dense(350, name='dense_4'))\n",
    "transfer_model.add(Activation('relu', name='activation_4'))\n",
    "\n",
    "transfer_model.add(Dense(4, name='dense_5'))\n",
    "transfer_model.add(Activation('softmax', name='activation_5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Fine tunning and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "transfer_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 200)           18000200  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 38, 600)           360600    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 36, 300)           540300    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 34, 150)           135150    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 32, 75)            33825     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               1440600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 350)               140350    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 1404      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 20,892,829\n",
      "Trainable params: 141,754\n",
      "Non-trainable params: 20,751,075\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../models/transfer_emo6_emo4/f1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "filepath = \"../models/transfer_emo6_emo4/f1/{epoch:02d}-{loss:0.3f}-{val_f1:0.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27144 samples, validate on 3016 samples\n",
      "Epoch 1/20\n",
      "27144/27144 [==============================] - 27s 990us/step - loss: 0.9760 - f1: 0.5299 - val_loss: 0.9430 - val_f1: 0.5609\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.97604, saving model to ../models/transfer_emo6_emo4/f1/01-0.976-0.561.hdf5\n",
      "Epoch 2/20\n",
      "27144/27144 [==============================] - 27s 992us/step - loss: 0.9545 - f1: 0.5427 - val_loss: 0.9430 - val_f1: 0.5573\n",
      "\n",
      "Epoch 00002: loss improved from 0.97604 to 0.95450, saving model to ../models/transfer_emo6_emo4/f1/02-0.954-0.557.hdf5\n",
      "Epoch 3/20\n",
      "27144/27144 [==============================] - 27s 992us/step - loss: 0.9452 - f1: 0.5498 - val_loss: 0.9271 - val_f1: 0.5533\n",
      "\n",
      "Epoch 00003: loss improved from 0.95450 to 0.94521, saving model to ../models/transfer_emo6_emo4/f1/03-0.945-0.553.hdf5\n",
      "Epoch 4/20\n",
      "27144/27144 [==============================] - 27s 997us/step - loss: 0.9463 - f1: 0.5464 - val_loss: 0.9319 - val_f1: 0.5618\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.94521\n",
      "Epoch 5/20\n",
      "27144/27144 [==============================] - 27s 1ms/step - loss: 0.9395 - f1: 0.5503 - val_loss: 0.9251 - val_f1: 0.5444\n",
      "\n",
      "Epoch 00005: loss improved from 0.94521 to 0.93947, saving model to ../models/transfer_emo6_emo4/f1/05-0.939-0.544.hdf5\n",
      "Epoch 6/20\n",
      "27144/27144 [==============================] - 28s 1ms/step - loss: 0.9404 - f1: 0.5468 - val_loss: 0.9263 - val_f1: 0.5467\n",
      "\n",
      "Epoch 00006: loss did not improve from 0.93947\n",
      "Epoch 7/20\n",
      "27144/27144 [==============================] - 28s 1ms/step - loss: 0.9365 - f1: 0.5522 - val_loss: 0.9269 - val_f1: 0.5506\n",
      "\n",
      "Epoch 00007: loss improved from 0.93947 to 0.93651, saving model to ../models/transfer_emo6_emo4/f1/07-0.937-0.551.hdf5\n",
      "Epoch 8/20\n",
      "27144/27144 [==============================] - 27s 994us/step - loss: 0.9318 - f1: 0.5545 - val_loss: 0.9250 - val_f1: 0.5589\n",
      "\n",
      "Epoch 00008: loss improved from 0.93651 to 0.93179, saving model to ../models/transfer_emo6_emo4/f1/08-0.932-0.559.hdf5\n",
      "Epoch 9/20\n",
      "27144/27144 [==============================] - 27s 994us/step - loss: 0.9294 - f1: 0.5562 - val_loss: 0.9245 - val_f1: 0.5561\n",
      "\n",
      "Epoch 00009: loss improved from 0.93179 to 0.92938, saving model to ../models/transfer_emo6_emo4/f1/09-0.929-0.556.hdf5\n",
      "Epoch 10/20\n",
      "27144/27144 [==============================] - 27s 993us/step - loss: 0.9339 - f1: 0.5532 - val_loss: 0.9233 - val_f1: 0.5558\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.92938\n",
      "Epoch 11/20\n",
      "27144/27144 [==============================] - 27s 1ms/step - loss: 0.9302 - f1: 0.5560 - val_loss: 0.9239 - val_f1: 0.5586\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.92938\n",
      "Epoch 12/20\n",
      "27144/27144 [==============================] - 27s 1ms/step - loss: 0.9287 - f1: 0.5569 - val_loss: 0.9194 - val_f1: 0.5474\n",
      "\n",
      "Epoch 00012: loss improved from 0.92938 to 0.92872, saving model to ../models/transfer_emo6_emo4/f1/12-0.929-0.547.hdf5\n",
      "Epoch 13/20\n",
      "27144/27144 [==============================] - 27s 1ms/step - loss: 0.9318 - f1: 0.5563 - val_loss: 0.9229 - val_f1: 0.5493\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.92872\n",
      "Epoch 14/20\n",
      "27144/27144 [==============================] - 28s 1ms/step - loss: 0.9290 - f1: 0.5549 - val_loss: 0.9211 - val_f1: 0.5565\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.92872\n",
      "Epoch 15/20\n",
      "27144/27144 [==============================] - 29s 1ms/step - loss: 0.9270 - f1: 0.5564 - val_loss: 0.9205 - val_f1: 0.5541\n",
      "\n",
      "Epoch 00015: loss improved from 0.92872 to 0.92702, saving model to ../models/transfer_emo6_emo4/f1/15-0.927-0.554.hdf5\n",
      "Epoch 16/20\n",
      "27144/27144 [==============================] - 28s 1ms/step - loss: 0.9285 - f1: 0.5569 - val_loss: 0.9195 - val_f1: 0.5580\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.92702\n",
      "Epoch 17/20\n",
      "27144/27144 [==============================] - 30s 1ms/step - loss: 0.9241 - f1: 0.5600 - val_loss: 0.9175 - val_f1: 0.5636\n",
      "\n",
      "Epoch 00017: loss improved from 0.92702 to 0.92411, saving model to ../models/transfer_emo6_emo4/f1/17-0.924-0.564.hdf5\n",
      "Epoch 18/20\n",
      "27144/27144 [==============================] - 31s 1ms/step - loss: 0.9244 - f1: 0.5568 - val_loss: 0.9181 - val_f1: 0.5603\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.92411\n",
      "Epoch 19/20\n",
      "27144/27144 [==============================] - 31s 1ms/step - loss: 0.9232 - f1: 0.5622 - val_loss: 0.9181 - val_f1: 0.5621\n",
      "\n",
      "Epoch 00019: loss improved from 0.92411 to 0.92315, saving model to ../models/transfer_emo6_emo4/f1/19-0.923-0.562.hdf5\n",
      "Epoch 20/20\n",
      "27144/27144 [==============================] - 31s 1ms/step - loss: 0.9222 - f1: 0.5603 - val_loss: 0.9175 - val_f1: 0.5600\n",
      "\n",
      "Epoch 00020: loss improved from 0.92315 to 0.92218, saving model to ../models/transfer_emo6_emo4/f1/20-0.922-0.560.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f870a8f0be0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "transfer_model.fit(tweets_emo_4, labels_emo_4, batch_size=128, epochs=20, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../models/transfer_emo6_emo4/entire_corpus/20-0.921-0.608-0.937-0.592.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "PATH_MODEL_EMO_4 = '../models/transfer_emo6_emo4/entire_corpus/20-0.921-0.608-0.937-0.592.hdf5'\n",
    "transfer_model = load_model(PATH_MODEL_EMO_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 200)           18000200  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 38, 600)           360600    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 36, 300)           540300    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 34, 150)           135150    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 32, 75)            33825     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               1440600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 350)               140350    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 1404      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 20,892,829\n",
      "Trainable params: 141,754\n",
      "Non-trainable params: 20,751,075\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../dataset/tweets/tweets_emotion_4/tweets_emotions_prediction-processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROCESSED_FILE = '../dataset/tweets/tweets_emotion_4/tweets_emotions_prediction-processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "Processing 2755/2755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2755/2755 [==============================] - 2s 622us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = transfer_model.predict(test_tweets, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = zip(map(str, range(len(test_tweets))), np.round(predictions[:, 0]).astype(int))\n",
    "utils.save_results_to_csv(results, '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36825687, 0.04995044, 0.5345072 , 0.04728551],\n",
       "       [0.02165285, 0.00963362, 0.75461566, 0.21409786],\n",
       "       [0.00128797, 0.0012112 , 0.436287  , 0.56121385],\n",
       "       ...,\n",
       "       [0.46889895, 0.263454  , 0.21271881, 0.05492825],\n",
       "       [0.14677207, 0.06191447, 0.69982505, 0.09148838],\n",
       "       [0.01749193, 0.00645487, 0.5592364 , 0.4168168 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusions = ['angry', 'sad', 'others', 'happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../dataset/tweets/tweets_emotion_4/tweets_emotions_prediction.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE_INPUT = '../dataset/tweets/tweets_emotion_4/tweets_emotions_prediction.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(TEST_FILE_INPUT, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    test_df[conclusions[i]] = predictions.T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>turn1</th>\n",
       "      <th>turn2</th>\n",
       "      <th>turn3</th>\n",
       "      <th>angry</th>\n",
       "      <th>sad</th>\n",
       "      <th>others</th>\n",
       "      <th>happy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Then dont ask me</td>\n",
       "      <td>YOURE A GUY NOT AS IF YOU WOULD UNDERSTAND</td>\n",
       "      <td>IM NOT A GUY FUCK OFF</td>\n",
       "      <td>0.368257</td>\n",
       "      <td>0.049950</td>\n",
       "      <td>0.534507</td>\n",
       "      <td>0.047286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mixed things  such as??</td>\n",
       "      <td>the things you do.</td>\n",
       "      <td>Have you seen minions??</td>\n",
       "      <td>0.021653</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.754616</td>\n",
       "      <td>0.214098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Today I'm very happy</td>\n",
       "      <td>and I'm happy for you ❤</td>\n",
       "      <td>I will be marry</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.436287</td>\n",
       "      <td>0.561214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Woah bring me some</td>\n",
       "      <td>left it there oops</td>\n",
       "      <td>Brb</td>\n",
       "      <td>0.125436</td>\n",
       "      <td>0.055360</td>\n",
       "      <td>0.559979</td>\n",
       "      <td>0.259225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>it is thooooo</td>\n",
       "      <td>I said soon master.</td>\n",
       "      <td>he is pressuring me</td>\n",
       "      <td>0.306928</td>\n",
       "      <td>0.166277</td>\n",
       "      <td>0.488765</td>\n",
       "      <td>0.038030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                    turn1                                       turn2  \\\n",
       "0   0         Then dont ask me  YOURE A GUY NOT AS IF YOU WOULD UNDERSTAND   \n",
       "1   1  Mixed things  such as??                          the things you do.   \n",
       "2   2     Today I'm very happy                     and I'm happy for you ❤   \n",
       "3   3       Woah bring me some                          left it there oops   \n",
       "4   4            it is thooooo                         I said soon master.   \n",
       "\n",
       "                     turn3     angry       sad    others     happy  \n",
       "0    IM NOT A GUY FUCK OFF  0.368257  0.049950  0.534507  0.047286  \n",
       "1  Have you seen minions??  0.021653  0.009634  0.754616  0.214098  \n",
       "2          I will be marry  0.001288  0.001211  0.436287  0.561214  \n",
       "3                      Brb  0.125436  0.055360  0.559979  0.259225  \n",
       "4      he is pressuring me  0.306928  0.166277  0.488765  0.038030  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE_OUTPUT = '../dataset/tweets/tweets_emotion_4/tweets_emotions_prediction_probabilities.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(TEST_FILE_OUTPUT, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = [conclusions[np.argmax(predictions[i])] for i in range(len(predictions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'happy',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'sad',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'sad',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'sad',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'happy',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'sad',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'angry',\n",
       " 'angry',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'others',\n",
       " 'sad',\n",
       " 'others',\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>turn1</th>\n",
       "      <th>turn2</th>\n",
       "      <th>turn3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Then dont ask me</td>\n",
       "      <td>YOURE A GUY NOT AS IF YOU WOULD UNDERSTAND</td>\n",
       "      <td>IM NOT A GUY FUCK OFF</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mixed things  such as??</td>\n",
       "      <td>the things you do.</td>\n",
       "      <td>Have you seen minions??</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Today I'm very happy</td>\n",
       "      <td>and I'm happy for you ❤</td>\n",
       "      <td>I will be marry</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Woah bring me some</td>\n",
       "      <td>left it there oops</td>\n",
       "      <td>Brb</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>it is thooooo</td>\n",
       "      <td>I said soon master.</td>\n",
       "      <td>he is pressuring me</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                    turn1                                       turn2  \\\n",
       "0   0         Then dont ask me  YOURE A GUY NOT AS IF YOU WOULD UNDERSTAND   \n",
       "1   1  Mixed things  such as??                          the things you do.   \n",
       "2   2     Today I'm very happy                     and I'm happy for you ❤   \n",
       "3   3       Woah bring me some                          left it there oops   \n",
       "4   4            it is thooooo                         I said soon master.   \n",
       "\n",
       "                     turn3   label  \n",
       "0    IM NOT A GUY FUCK OFF  others  \n",
       "1  Have you seen minions??  others  \n",
       "2          I will be marry   happy  \n",
       "3                      Brb  others  \n",
       "4      he is pressuring me  others  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE_OUTPUT = '../results/tweets_emotions_predictions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(TEST_FILE_OUTPUT, sep='\\t', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
